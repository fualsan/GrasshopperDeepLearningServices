from diffusers import AutoPipelineForText2Image
from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler
from transformers import pipeline
from controlnet_aux import PidiNetDetector, HEDdetector

import torch

import numpy as np
import cv2

from fastapi import FastAPI, Request
from pydantic import BaseModel, Field
from PIL import Image
from io import BytesIO
import base64
import uvicorn
import gc
import logging
import os
from time import time_ns

device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f'Using device: {device}')

# this folder is generated by Dockerfile
# however, standalone runs might need it
os.makedirs('./generated_images/', exist_ok=True)

################# LOGGER ###################
logger = logging.getLogger('ghdls')
logger.setLevel(logging.DEBUG)

# print to a log file
file_handler = logging.handlers.RotatingFileHandler(
    filename='ghdls.log',
    encoding='utf-8',
    maxBytes=32 * 1024 * 1024, # 32 MB
    backupCount=5,  # Rotate through 5 files
)

# hour minute, seconds, day, month, year
log_format = '%H:%M:%S %d-%m-%Y'
formatter = logging.Formatter('[{asctime}] [{levelname:<8}] {name}: {message}', log_format, style='{')
file_handler.setFormatter(formatter)
logger.addHandler(file_handler)
file_handler.setLevel(logging.DEBUG)
file_handler.setFormatter(formatter)

# print to terminal (console)
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.DEBUG)
console_handler.setFormatter(formatter)

logger.addHandler(file_handler)
logger.addHandler(console_handler)
############################################


############## TEXT 2 IMAGE ################
print('LOADING TXT2IMG MODEL...')

t2i_pipeline = AutoPipelineForText2Image.from_pretrained(
	'CompVis/stable-diffusion-v1-4', 
    cache_dir='/workspace/hub',
    torch_dtype=torch.float16, 
    variant='fp16', 
    use_safetensors=True
).to(device)


t2i_pipeline.enable_model_cpu_offload()
#t2i_pipeline.enable_xformers_memory_efficient_attention()

print('TXT2IMG MODEL LOADED SUCCESSFULLY!')
############################################


############## IMAGE 2 IMAGE ###############
SUPPORTED_NAMES = (
	'depth_default',
	'canny',
	'softedge',
	'scribble'
)

def load_depth_default():
	control_img_generator = pipeline('depth-estimation', cache_dir='/workspace/hub')

	def preprocess_control_img(control_img_generator, image):
		image = control_img_generator(image)['depth']
		image = np.array(image)
		image = image[:, :, None]
		image = np.concatenate([image, image, image], axis=2)
		image = Image.fromarray(image)
		# save image (OPTIONAL)
		#image.save(f'./generated_images/control_img_controlnet_{time_ns()}.jpg')
		return image

	controlnet = ControlNetModel.from_pretrained(
		'lllyasviel/sd-controlnet-depth', 
        cache_dir='/workspace/hub',
		torch_dtype=torch.float16
	)

	i2i_pipeline = StableDiffusionControlNetPipeline.from_pretrained(
		'CompVis/stable-diffusion-v1-4', 
		cache_dir='/workspace/hub',
        controlnet=controlnet, 
		safety_checker=None, 
		torch_dtype=torch.float16,
		variant='fp16'
	).to(device)

	i2i_pipeline.scheduler = UniPCMultistepScheduler.from_config(i2i_pipeline.scheduler.config)

	#i2i_pipeline.enable_xformers_memory_efficient_attention()
	i2i_pipeline.enable_model_cpu_offload()

	return control_img_generator, preprocess_control_img, controlnet, i2i_pipeline


def load_canny():
	# Another pipeline is not required for canny
	# However, this is kept for compatibility
	control_img_generator = None

	# TODO: take low_threshold, high_threshold from request
	def preprocess_control_img(control_img_generator, image, low_threshold=100, high_threshold=200):
		image = np.array(image)
		image = cv2.Canny(image, low_threshold, high_threshold)
		image = image[:, :, None]
		image = np.concatenate([image, image, image], axis=2)
		image = Image.fromarray(image)

		# save image (OPTIONAL)
		#image.save(f'./generated_images/control_img_controlnet_{time_ns()}.jpg')
		return image

	controlnet = ControlNetModel.from_pretrained(
		'lllyasviel/sd-controlnet-canny', 
        cache_dir='/workspace/hub',
		torch_dtype=torch.float16,
		#variant='fp16',
	)

	i2i_pipeline = StableDiffusionControlNetPipeline.from_pretrained(
		'CompVis/stable-diffusion-v1-4',
		cache_dir='/workspace/hub',
        controlnet=controlnet, 
		safety_checker=None, 
		torch_dtype=torch.float16,
		variant='fp16',
	).to(device)

	i2i_pipeline.scheduler = UniPCMultistepScheduler.from_config(i2i_pipeline.scheduler.config)

	#i2i_pipeline.enable_xformers_memory_efficient_attention()
	i2i_pipeline.enable_model_cpu_offload()

	return control_img_generator, preprocess_control_img, controlnet, i2i_pipeline


def load_softedge():

	control_img_generator = PidiNetDetector.from_pretrained('lllyasviel/Annotators', cache_dir='/workspace/hub')

	# Not very useful in this case
	# However, this is kept for compatibility
	def preprocess_control_img(control_img_generator, image):
		image = control_img_generator(image, safe=True)

		# save image (OPTIONAL)
		#image.save(f'./generated_images/control_img_controlnet_{time_ns()}.jpg')
		return image

	controlnet = ControlNetModel.from_pretrained(
		'lllyasviel/control_v11p_sd15_softedge',
        cache_dir='/workspace/hub',
		torch_dtype=torch.float16
	)

	i2i_pipeline = StableDiffusionControlNetPipeline.from_pretrained(
		'CompVis/stable-diffusion-v1-4', 
		cache_dir='/workspace/hub',
        controlnet=controlnet, 
		torch_dtype=torch.float16
	).to(device)

	i2i_pipeline.scheduler = UniPCMultistepScheduler.from_config(i2i_pipeline.scheduler.config)

	#i2i_pipeline.enable_xformers_memory_efficient_attention()
	i2i_pipeline.enable_model_cpu_offload()

	return control_img_generator, preprocess_control_img, controlnet, i2i_pipeline


def load_scribble():

	control_img_generator = HEDdetector.from_pretrained('lllyasviel/ControlNet', cache_dir='/workspace/hub')

	# Not very useful in this case
	# However, this is kept for compatibility
	def preprocess_control_img(control_img_generator, image):
		image = control_img_generator(image, scribble=True)

		# save image (OPTIONAL)
		#image.save(f'./generated_images/control_img_controlnet_{time_ns()}.jpg')
		return image

	controlnet = ControlNetModel.from_pretrained(
		'lllyasviel/sd-controlnet-scribble',
        cache_dir='/workspace/hub',
		torch_dtype=torch.float16
	)

	i2i_pipeline = StableDiffusionControlNetPipeline.from_pretrained(
		'CompVis/stable-diffusion-v1-4', 
		cache_dir='/workspace/hub',
        controlnet=controlnet, 
		torch_dtype=torch.float16
	).to(device)

	i2i_pipeline.scheduler = UniPCMultistepScheduler.from_config(i2i_pipeline.scheduler.config)

	#i2i_pipeline.enable_xformers_memory_efficient_attention()
	i2i_pipeline.enable_model_cpu_offload()

	return control_img_generator, preprocess_control_img, controlnet, i2i_pipeline
############################################


############### LOAD DEFAULT ###############
print('LOADING IMG2IMG MODEL...')

# This is the defaul model/pipeline loaded when service starts
control_img_generator, preprocess_control_img, controlnet, i2i_pipeline = load_depth_default()

print('IMG2IMG MODEL LOADED SUCCESSFULLY!')
############################################


# uvicorn runs this
app = FastAPI()


class Text2ImageRequest(BaseModel):
	prompt: str
	negative_prompt: str = Field(default=None) # TODO: max_length ?
	seed: int = Field(default=1234, gt=0, description='Seed must be greater than zero')
	guidance_scale: float = Field(default=7.5, ge=0.0, description='Guidence scale must be greater than or equal to zero')
	height: int = Field(default=512, gt=0, description='Height must be greater than zero')
	width: int = Field(default=512, gt=0, description='Width must be greater than zero')
	num_inference_steps: int = Field(default=50, gt=0, description='Number of inference steps scale must be greater than zero')


class Image2ImageRequest(BaseModel):
	image: str # base64 encoded image as string
	prompt: str
	negative_prompt: str = Field(default=None) # TODO: max_length ?
	seed: int = Field(default=1234, gt=0, description='Seed must be greater than zero')
	guidance_scale: float = Field(default=7.5, ge=0.0, description='Guidence scale must be greater than or equal to zero')
	# a lower strength value means the generated image is more similar to the initial image
	strength: float = Field(default=0.8, ge=0.0, description='Strength scale must be greater than or equal to zero')
	height: int = Field(default=512, gt=0, description='Height must be greater than zero')
	width: int = Field(default=512, gt=0, description='Width must be greater than zero')
	num_inference_steps: int = Field(default=50, gt=0, description='Number of inference steps scale must be greater than zero')


class LoadRequest(BaseModel):
	name: str # name of the model/pipeline
	reset: bool = Field(default=False) # removes previously loaded


@app.post('/t2i')
async def text2image_generation(request: Text2ImageRequest):

	generator = torch.Generator(device).manual_seed(request.seed)

	# TODO: multiple images maybe? 
	image = t2i_pipeline(
		prompt=request.prompt, 
		negative_prompt=request.negative_prompt, 
		generator=generator, 
		guidance_scale=request.guidance_scale, 
		height=request.height, 
		width=request.height,
		num_inference_steps=request.num_inference_steps
	).images[0]

	# save image (OPTIONAL)
	#image.save(f'./generated_images/gen_img_{time_ns()}.jpg')

	buffer = BytesIO()
	image.save(buffer, format='JPEG')
	generated_image = base64.b64encode(buffer.getvalue()).decode('utf-8')

	return {'generated_image': generated_image}


@app.post('/i2i')
async def image2image_generation(request: Image2ImageRequest):

	generator = torch.Generator(device).manual_seed(request.seed)

	# decode base64 encoded low res image
	decoded_image = base64.b64decode(request.image, validate=True)
	decoded_image = Image.open(BytesIO(decoded_image))#.convert('RGB')

	# TODO: multiple images maybe? 
	depth_image = preprocess_control_img(control_img_generator, decoded_image)

	image = i2i_pipeline(
		image=depth_image,
		prompt=request.prompt, 
		negative_prompt=request.negative_prompt, 
		generator=generator, 
		guidance_scale=request.guidance_scale, 
		strength=request.strength,
		height=request.height, 
		width=request.height,
		num_inference_steps=request.num_inference_steps
	).images[0]
	
	# save image (OPTIONAL)
	#image.save(f'./generated_images/gen_img_{time_ns()}.jpg')

	buffer = BytesIO()
	image.save(buffer, format='JPEG')
	generated_image = base64.b64encode(buffer.getvalue()).decode('utf-8')

	return {'generated_image': generated_image}


@app.post('/load')
async def load_model(request: LoadRequest):
	global control_img_generator, preprocess_control_img, controlnet, i2i_pipeline

	_name = request.name

	if _name not in SUPPORTED_NAMES:
		_supported_names_str = ', '.join(SUPPORTED_NAMES) 
		return {'result': f'{_name} is not supported! Please select from: {_supported_names_str}'}

	# OPTIONAL
	if request.reset:
		reset_models()

	if _name == 'depth_default':
		control_img_generator, preprocess_control_img, controlnet, i2i_pipeline = load_depth_default()
	elif _name == 'canny':
		control_img_generator, preprocess_control_img, controlnet, i2i_pipeline = load_canny()
	elif _name == 'softedge':
		control_img_generator, preprocess_control_img, controlnet, i2i_pipeline = load_softedge()
	elif _name == 'scribble':
		control_img_generator, preprocess_control_img, controlnet, i2i_pipeline = load_scribble()

	return {'result': f'{_name} loaded succesfully!'}


@app.post('/reset_all')
async def reset_models(request: Request):
	global control_img_generator, i2i_pipeline, t2i_pipeline

	if control_img_generator is not None:
		del control_img_generator
		control_img_generator = None
	else:
		print('Depth estimator already removed from memory!')

	if t2i_pipeline is not None:
		del t2i_pipeline
		t2i_pipeline = None
	else:
		print('Text to image already removed from memory!')

	if i2i_pipeline is not None:
		del i2i_pipeline
		i2i_pipeline = None
	else:
		print('Image to image already removed from memory!')

	gc.collect()
	torch.cuda.empty_cache()

	return {'result': 'all removed from memory!'}


@app.middleware('http')
async def log_request_info(request, call_next):
	"""
	request.client.host
    request.client.port
    request.method
    request.url
    request.url.path
    request.query_params
    request.headers
	"""
	logger.debug(f'{request.url.path} endpoint received {request.method} request from {request.client.host}:{request.client.port} using agent: {request.headers["user-agent"]}')

	response = await call_next(request)
	return response


if __name__ == '__main__':
	logger.debug(f'Grasshopper Deep Learning Services (GHDLS) is starting...')
	# accept every connection (not only local connections)
	uvicorn.run(app, host='0.0.0.0', port=9999)
