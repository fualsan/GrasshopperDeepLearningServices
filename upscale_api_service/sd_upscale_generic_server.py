from diffusers import StableDiffusionLatentUpscalePipeline, StableDiffusionUpscalePipeline
import torch

from fastapi import FastAPI
from pydantic import BaseModel, Field
from PIL import Image
from io import BytesIO
import base64
import uvicorn
import logging
import os
from time import time_ns

device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f'Using device: {device}')


# this folder is generated by Dockerfile
# however, standalone runs might need it
os.makedirs('./generated_images/', exist_ok=True)


################# LOGGER ###################
logger = logging.getLogger('sd_upscale_generic_server')
logger.setLevel(logging.DEBUG)

# print to a log file
file_handler = logging.handlers.RotatingFileHandler(
    filename='sd_upscale_generic_server.log',
    encoding='utf-8',
    maxBytes=32 * 1024 * 1024, # 32 MB
    backupCount=5,  # Rotate through 5 files
)

# hour minute, seconds, day, month, year
log_format = '%H:%M:%S %d-%m-%Y'
formatter = logging.Formatter('[{asctime}] [{levelname:<8}] {name}: {message}', log_format, style='{')
file_handler.setFormatter(formatter)
logger.addHandler(file_handler)
file_handler.setLevel(logging.DEBUG)
file_handler.setFormatter(formatter)

# print to terminal (console)
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.DEBUG)
console_handler.setFormatter(formatter)

logger.addHandler(file_handler)
logger.addHandler(console_handler)
############################################


############## 2x UPSCALING ###############
pipeline_2x = StableDiffusionLatentUpscalePipeline.from_pretrained(
	'stabilityai/sd-x2-latent-upscaler', 
	use_safetensors=True,
).to(device)


pipeline_2x.enable_model_cpu_offload()
#pipeline_2x.enable_xformers_memory_efficient_attention()
############################################


############## 4x UPSCALING ###############
pipeline_4x = StableDiffusionUpscalePipeline.from_pretrained(
	'stabilityai/stable-diffusion-x4-upscaler', 
	use_safetensors=True, 
	torch_dtype=torch.float16, 
	variant='fp16'
).to(device)


pipeline_4x.enable_model_cpu_offload()
#pipeline_4x.enable_xformers_memory_efficient_attention()
############################################


# uvicorn runs this
app = FastAPI()


class UpscaleRequest(BaseModel):
	image: str # base64 encoded image as string
	prompt: str = Field(default='')
	negative_prompt: str = Field(default=None) # TODO: max_length ?
	seed: int = Field(default=1234, gt=0, description='Seed must be greater than zero')
	# NOTE: guidance_scale is set to 0.0 for pure upscaling
	guidance_scale: float = Field(default=0.0, ge=0, description='Guidence scale must be greater than or equal to zero')
	num_inference_steps: int = Field(default=50, gt=0, description='Number of inference steps scale must be greater than zero')


@app.post('/upscale2x')
async def process_image(request: UpscaleRequest):

	generator = torch.Generator(device).manual_seed(request.seed)

	# decode base64 encoded low res image
	decoded_image = base64.b64decode(request.image, validate=True)
	decoded_image = Image.open(BytesIO(decoded_image))

	# TODO: multiple images maybe? 
	image = pipeline_2x(
		image=decoded_image,
		prompt=request.prompt, 
		negative_prompt=request.negative_prompt, 
		generator=generator, 
		guidance_scale=request.guidance_scale,
		num_inference_steps=request.num_inference_steps
	).images[0]

	# save image (OPTIONAL)
	#image.save('/tmp/gen_img.jpg')

	buffer = BytesIO()
	image.save(buffer, format='JPEG')
	generated_image = base64.b64encode(buffer.getvalue()).decode('utf-8')

	return {'generated_image': generated_image}


@app.post('/upscale4x')
async def process_image(request: UpscaleRequest):

	generator = torch.Generator(device).manual_seed(request.seed)

	# decode base64 encoded low res image
	decoded_image = base64.b64decode(request.image, validate=True)
	decoded_image = Image.open(BytesIO(decoded_image))

	# TODO: multiple images maybe? 
	image = pipeline_4x(
		image=decoded_image,
		prompt=request.prompt, 
		negative_prompt=request.negative_prompt, 
		generator=generator, 
		guidance_scale=request.guidance_scale,
		num_inference_steps=request.num_inference_steps
	).images[0]

	# save image (OPTIONAL)
	#image.save('/tmp/gen_img.jpg')

	buffer = BytesIO()
	image.save(buffer, format='JPEG')
	generated_image = base64.b64encode(buffer.getvalue()).decode('utf-8')

	return {'generated_image': generated_image}


@app.middleware('http')
async def log_request_info(request, call_next):
	"""
	request.client.host
    request.client.port
    request.method
    request.url
    request.url.path
    request.query_params
    request.headers
	"""
	logger.debug(f'{request.url.path} endpoint received {request.method} request from {request.client.host}:{request.client.port} using agent: {request.headers["user-agent"]}')

	response = await call_next(request)
	return response


if __name__ == '__main__':
    logger.debug(f'SD Upscale Grasshopper Deep Learning Services (GHDLS) is starting...')
	# accept every connection (not only local connections)
    uvicorn.run(app, host='0.0.0.0', port=9977)